### CCC and the Maximal Information Coefficient (MIC)

#### Conceptual and statistical differences

The Clustermatch Correlation Coefficient (CCC) and the Maximal Information Coefficient (MIC) [@pmid:22174245] (see [Methods](#sec:mic)) are measures designed to capture non-linear relationships between variables.
While they share certain similarities, there are also notable differences between them.

Conceptually, CCC is grounded in clustering input data using each variable separately.
This process effectively transforms each variable into a set of partitions, each containing a different number of clusters.
The CCC then quantifies the correlation between variables by assessing the similarity of these partitions.
This allows to process of various types of variables, including both numerical and categorical variables, even when the categories are nominal (i.e., they lack intrinsic order), as explained in [Methods](#sec:ccc_algo).
MIC, however, is specifically designed for numerical variables.
Additionally, in theory, CCC should also support correlating variables with different dimensions.
For 1-dimensional variables (such as genes), CCC obtains partitions using a quantiles-based approach.
For multidimensional variables, CCC could potentially use a standard clustering algorithm (such as $k$-means) to obtain partitions.

Now, consider two variables with $n$ data points on a scatterplot.
We can overlay a grid on this scatterplot with $x$ columns and $y$ rows, where each cell of this grid contains a portion of the data points, thereby defining a bivariate probability distribution.
The MIC algorithm seeks an optimal grid configuration that maximizes the ratio of mutual information to $\log \min \{x, y\}$, subject to the constraint that $xy < n^{0.6}$.
This normalization process using $\log \min \{x, y\}$ scales the MIC score between zero and one.
The CCC, as defined in [Methods](#sec:ccc_algo), also generates a symmetric, normalized score between zero and one.
However, unlike MIC which utilizes normalized mutual information, CCC employs the Adjusted Rand Index (ARI).
The ARI has an advantageous property: it consistently returns a baseline (zero) for independently drawn partitions, irrespective of the number of clusters (see Figure @fig:constant_baseline:k_max).
This property is not inherent in mutual information, which can produce varied values for independent variables if the grid dimensions vary.
MIC mitigates this by limiting the grid size with the constraint $xy < n^{0.6}$, which could also limit its ability to detect complex relationships.

Both CCC and MIC involve binning the input data vectors, aiming to maximize the mutual information and the ARI, respectively.
However, their approaches differ in complexity and execution.
MIC utilizes a sophisticated dynamic programming algorithm to identify the optimal grid.
In contrast, CCC employs a more straightforward and faster method, partitioning the data points separately using the two vectors.
While CCC might benefit from adopting MIC's more complex grid search approach, it remains uncertain if MIC could maintain its performance using CCC's simpler partitioning strategy.

Regarding their parameters, CCC's $k_{\mathrm{max}}$ (maximum number of clusters) and MIC's $B(n)$ (maximum grid size) serve similar purposes.
They control both the complexity of the patterns detected and the computational time.
For example, as illustrated in Figure @fig:datasets_rel (Anscombe I and III), a $k_{\mathrm{max}}$ of 2 is adequate for identifying linear patterns but insufficient for more complex patterns like quadratic or two-lines patterns.
A similar principle applies to MIC's $B(n)$.
However, a critical distinction exists between the two: the constant baseline property of ARIs ensures that CCC returns a value close to zero for independent variables, regardless of $k_{\mathrm{max}}$.
In contrast, MIC may produce non-zero scores for independent data if $B(n)$ is set too high, as discussed in Section 2.2.1 of the supplementary material in [@pmid:22174245].
The authors of MIC suggest that a value of $B(n) = n^{0.6}$ is generally effective in practice.

#### Comparison in gene expression data

We compared all the coefficients in this study with MIC, a popular nonlinear method that can find complex relationships in data, although very computationally intensive [@doi:10.1098/rsos.201424].
We ran MIC<sub>e</sub> (see Methods) on all possible pairwise comparisons of our 5,000 highly variable genes from whole blood in GTEx v8.
<!-- This took 4 days and 19 hours to finish (compared with 9 hours for CCC). -->
Then we performed the analysis on the distribution of coefficients (the same as in the main text), shown in Figure @fig:dist_coefs_mic.
We verified that CCC and MIC behave similarly in this dataset, with essentially the same distribution but only shifted.
Figure @{fig:dist_coefs_mic}c shows that these two coefficients relate almost linearly, and both compare very similarly with Pearson and Spearman.

![
**Distribution of MIC values on gene expression (GTEx v8, whole blood)  and comparison with other methods.**
**a)** Histogram of coefficient values.
**b)** Corresponding cumulative histogram. The dotted line maps the coefficient value that accumulates 70% of gene pairs.
**c)** 2D histogram plot with hexagonal bins between all coefficients, where a logarithmic scale was used to color each hexagon.
](images/coefs_comp/gtex_whole_blood/mic/dist-main.svg "Distribution of MIC values"){#fig:dist_coefs_mic width="100%"}
