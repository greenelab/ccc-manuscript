### A robust and efficient not-only-linear dependence coefficient

Clustermatch is a dependance coefficient that can compute a similarity measure between any pair of variables, either with numerical or categorical values [@doi:10.1093/bioinformatics/bty899].
The method assumes that if there is a relationship between two variables/features describing $n$ data points/objects, then the clusterings on those $n$ objects derived using each variable individually should match (see Methods).
Although different clustering algorithms can be optimally applied to one-dimensional data (such as using kernel density estimation or the Jenks Natural Breaks optimization method [@Jenks1967]), we used quantiles to separate data points into different clusters (i.e., the median separates numerical data into two clusters).
Since in Clustermatch the data is categorized into clusters, numerical and categorical data can be naturally integrated since clusters do not need an order.
Once all internal partitions from each variable are generated, the Clustermatch coefficient is defined as the maximum adjusted Rand index (ARI) [@doi:10.1007/BF01908075] between them.
We previously compared Clustermatch [@doi:10.1093/bioinformatics/bty899] with the Maximal Information Coefficient (MIC) [@doi:10.1126/science.1205438] and Distance Correlation (DC) [@doi:10.1214/009053607000000505], two popular nonlinear coefficients.
In addition to outperforming these two methods in a simulated scenario with different noise levels, Clustermatch was also significantly superior on computational complexity, making it the only practical not-only-linear coefficient for real and large datasets such as gene expression compendia.
Therefore, in this study we will only focus on Clustermatch and two widely used Pearson, Spearman and Clustermatch.


![
**Different types of relationships in data.**
Each panel contains a set of simulated data points described by two variables: $x$ and $y$.
The first row shows the Anscombe's quartet with four different datasets (from Anscombe I to IV) with 11 data points each.
The second row contains another set of general patterns with 100 data points each.
Each panel shows the correlation value using the Pearson ($r$), Spearman ($r_s$) and Clustermatch ($c$) coefficients.
Vertical and horizontal lines show how Clustermatch separated data points using $x$ and $y$, respectively.
](images/intro/relationships.svg "Different types of relationships in data"){#fig:datasets_rel width="100%"}


In Figure @fig:datasets_rel, we show how Pearson ($r$), Spearman ($r_s$) and Clustermatch ($c$) behave on different data patterns, where red lines indicate how Clustermatch clusters data points using each feature individually (either $x$ or $y$).
In the first row of the figure, the Anscombe's quartet [@doi:10.1080/00031305.1973.10478966] is shown, which comprises four synthetic datasets with completely different patterns but exactly the same data statistics (mean, standard deviation and Pearson's correlation).
This kind of simulated data, including also the "Datasaurus" [@url:http://www.thefunctionalart.com/2016/08/download-datasaurus-never-trust-summary.html; @doi:10.1145/3025453.3025912; @doi:10.1111/dsji.12233], are frequently used as a reminder of the importance of going beyond simple statistics, where either undesirable patterns (such as outliers) or desirable ones (such as non-linear relationships reflecting real and complex biological relationships) can be masked by these numbers.
For example, Anscomble I seems to show a noisy but clear linear pattern, similar to Anscombe III where the linearity is perfect besides one "outlier".
For these two patterns, Clustermatch separates these data points using two clusters (one red line for each variable $x$ and $y$), yielding 1.0, the maximum value, correctly identying the relationship.
Anscombe II seems to follow a quadratic distribution, and this is reflected in the Clustermatch with a lower yet non-zero value of 0.34.
Anscombe IV shows a vertical line where $x$ values are almost contant except for one "outlier".
This outlier does not influece Clustermatch as it does for Pearson or Spearman, and thus $c$=0.00 (the minimim value) indicates no association for this variable pair because it does not fit the Clustermatch assumption: the two clusters formed with $x$ (approximately separated by $x=13$) do not match well the three clusters formed with $y$.
The Pearson's correlation coefficient is the same across all these Anscombe's examples ($r$=0.82) whereas Spearman is always above or equal to 0.50.


The second row of Figure @fig:datasets_rel shows other simulated relationships with general nonlinear patterns, some of which were previously observed in gene expression data [@doi:10.1126/science.1205438; @doi:10.3389/fgene.2019.01410; @doi:10.1091/mbc.9.12.3273].
For the random/independent pair of variables, all coefficients correctly agree with a value close to zero.
In this case, Clustermatch separates data points into five clusters using $x$ and two using $y$, which do not match thus yielding $c$=0.01.
For the other three examples (quadratic, noncoexistence and two lines), Pearson and Spearman generally fail to capture a clear pattern between variables $x$ and $y$.
These patterns also show how Clustermatch uses different degrees of complexity to capture the relationships.
For the noncoexistence pattern, where for instance one gene ($x$) might be expressed whilethe other one ($y$) is inhibited, Clustermatch only needs two clusters for both variables, similarly to a linear relationship (Anscombe I and III), which also explains why Pearson and Spearman yield high values here as well.
For the quadratic pattern, Clustermatch needs to separate $x$ into more clusters (four in this case) to reach the maximum ARI.
The two lines example shows two embedded linear relationships, not detected by either Pearson or Spearman, and for which Clustermatch needs eight clusters for $x$ and six for $y$.


Datasets such as Anscombe or "Datasaurus" highlight the need of visualization before drawing conclusions on summary statistics alone.
Although extra steps such as a visual analysis are always mandatory, larger datasets make it impossible to perform manual assessment on each, for example, gene pair.
More advanced techniques, such as Clustermatch, could reduce the number of false positives/negatives to focus human validation on patterns that are more likely to be real.
Clustermatch has only one parameter: $k$, which is 10 by default, is the maximum number of internal clusters that the algorithm will use when partitioning data points using each variable.
As we showed in the examples above, this parameter can control the level of complexity the end-user desires to capture.
A value of $k$=2 makes the coefficient more leaned towards linear patterns, whereas higher values can detect other, more complex kinds of relationships.
We found that $k$=10 approximates well the coefficient values for different types of patterns [@doi:10.1093/bioinformatics/bty899] while balancing computing time and keeping a close-to-zero value for random data, which is guaranteed by the adjusted-for-chance property of ARI [@Vinh2010].


In the next sections we compare there coefficients on real gene expression data and highlight some complex and potentially interesting relationships.
