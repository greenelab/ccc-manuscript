### Overview of CCC: a not-only-linear correlation coefficient

![
**Different patterns across data types.**
Each panel contains a set of simulated data points described by a pair of generic variables: $x$ and $y$ are numerical, whereas $w$ represent two categorical values ("Orange" and "Blue") and $z$ three categorical values ("A", "B" and "C").
**a)** The Anscombe's quartet with four different datasets (from Anscombe I to IV) and 11 data points each.
**b)** A set of patterns between two numerical variables with 100 data points.
**c)** A set of patterns between two categorical variables with 100 data points.
**d)** A set of patterns between one categorical and one numerical variable with 100 data points.
Each panel shows the correlation value using: Pearson ($p$) and Spearman ($s$) for numerical variables only, and CCC ($c$) for both numerical and categorical;
their statistical significance is indicated with asterisks: $P<0.05$ (\*), $P<0.01$ (\*\*), and $P<0.001$ (\*\*\*).
For CCC, we computed the p-value using 10,000 permutations.
Vertical and horizontal red lines show how CCC clustered data points using $x$ and/or $y$.
For categorical variables, CCC uses the categories to cluster data points.
](images/intro/relationships.svg "Different patterns across data types"){#fig:datasets_rel width="100%"}


The CCC provides a similarity measure between any pair of variables, either with numerical or categorical values.
The method assumes that if there is a relationship between two variables/features describing $n$ data points/objects, then the **cluster**ings of those objects using each variable should **match**.
In the case of numerical values, CCC uses quantiles to efficiently separate data points into different clusters (e.g., the median separates numerical data into two clusters).
For categorical values, CCC uses the categories themselves to separate data points into different clusters (e.g., if feature "color" has three values, "red", "green", and "blue", then data will be clustered into three clusters defined by those colors).
Once all clusterings are generated according to each variable, we define the CCC as the maximum adjusted Rand index (ARI) [@doi:10.1007/BF01908075] between them, ranging between 0 and 1.
Details of the CCC algorithm can be found in [Methods](#sec:ccc_algo).


We examined how the Pearson ($p$), Spearman ($s$) and CCC ($c$) correlation coefficients behaved on different simulated data patterns.
Figure @fig:datasets_rel shows different types of relationships between two variables of different data types, where $x$ and $y$ are numerical and $w$ and $z$ are categorical.
For each variable pair, we show the coefficient values and their statistical significance, where asterisks indicate different $P$-values ($P$).
The red lines show how CCC clustered numerical data points using $x$ (vertical lines) and $y$ (horizontal lines).


In Figure @{fig:datasets_rel}a, we examine the classic Anscombe's quartet [@doi:10.1080/00031305.1973.10478966], which comprises four synthetic datasets with different patterns but the same data statistics (mean, standard deviation and Pearson's correlation).
This kind of simulated data, recently revisited with the "Datasaurus" [@url:http://www.thefunctionalart.com/2016/08/download-datasaurus-never-trust-summary.html; @doi:10.1145/3025453.3025912; @doi:10.1111/dsji.12233], is used as a reminder of the importance of going beyond simple statistics, where either undesirable patterns (such as outliers) or desirable ones (such as biologically meaningful nonlinear relationships) can be masked by summary statistics alone.
Anscombe I contains a noisy but clear linear pattern, similar to Anscombe III where the linearity is perfect besides one outlier.
In these two examples, CCC separates data points using two clusters (one red line for each variable $x$ and $y$), yielding a statistically significant value of 1.0 (the maximum for CCC) and thus indicating a strong relationship.
Anscombe II seems to follow a partially quadratic relationship interpreted as linear by Pearson and Spearman.
In contrast, for this potentially undersampled quadratic pattern, CCC yields a lower and not statistically significant value of 0.34, reflecting a more complex relationship than a linear pattern.
Anscombe IV shows a vertical line of data points where $x$ values are almost constant except for one outlier.
This outlier does not influence CCC as it does for Pearson or Spearman, although only Pearson yields a statistically significant result.
Thus $c=0.00$ (the minimum value) correctly indicates no association for this variable pair because, besides the outlier, for a single value of $x$ there are ten different values for $y$.
This pair of variables does not fit the CCC assumption: the two clusters formed with $x$ (approximately separated by $x=13$) do not match the three clusters formed with $y$.
The Pearson's correlation coefficient is the same across all these Anscombe's examples ($p=0.82$), whereas Spearman is 0.50 or greater.


We also simulated additional types of numerical relationships (Figure @{fig:datasets_rel}b), including some previously described from gene expression data [@doi:10.1126/science.1205438; @doi:10.3389/fgene.2019.01410; @doi:10.1091/mbc.9.12.3273].
For the random/independent pair of variables, all coefficients correctly agree with a value close to zero and $P>0.05$.
The non-coexistence pattern, captured by all coefficients, represents a case where one gene ($x$) might be expressed while the other one ($y$) is inhibited, highlighting a potentially strong biological relationship (such as a microRNA negatively regulating another gene).
For the other two examples (quadratic and two-lines), only CCC is able to yield a high and statistically significant correlation value, whereas Pearson and Spearman fail to capture these nonlinear pattern.
These relationships also show how CCC uses different degrees of complexity to capture the relationships.
For the quadratic pattern, for example, CCC separates $x$ into more clusters (four in this case) to reach the maximum ARI.
The two-lines example shows two embedded linear relationships with different slopes, which neither Pearson nor Spearman detect ($p=-0.12$ and $s=0.05$, respectively, both with $P>0.05$).
Here, CCC increases the complexity of the model by using eight clusters for $x$ and six for $y$, resulting in $c=0.31$ ($P<0.001$).


Finally, we also simulated categorical variables, which only CCC can handle.
Figure @{fig:datasets_rel}c shows two patterns between variables $w$ (with categories "Orange" and "Blue") and $z$ (with categories "A", "B" and "C").
The first case (Two-Categorical I) represents a random/independent pattern where categorical values in one variable are approximately uniformly distributed across the categorical values of the other variable.
Here, as expected, CCC yield a very low and non-significant value.
In the second case (Two-Categorical II), the category "Orange" of $w$ is overrepresented in data points with $z$ equal to "B" and, less strongly, the category "Blue" of $w$ is overrepresented in data points with $z$ equal to "A".
In this case, since CCC clusters data points using the categorical values, it detects that clusters of data points with $w$="Orange" match clusters with $z$="B", yielding a statistically significant $c=0.11$.
Figure @{fig:datasets_rel}d mixes a categorical variable ($z$) with a numerical one ($y$).
The first case (Categorical-Numerical I) represents a random/independent pattern where numerical values in $y$ are approximately uniformly distributed across the categorical values in $z$.
Similarly as in the other random/independent cases, CCC yields a very low and non-significant value, since the clusters formed by $y$ do not match the clusters (given by the categorical values) formed by $z$.
Conversely, in the second case (Categorical-Numerical II), clusters of data points with similar values in $y$ tend to have also similar categorical values in $z$.
In this example, for data points with $z$="A", we assigned $y \sim \mathcal{N}(0, 0.5^2)$, whereas for $z$="B" and "C", we assigned $y \sim \mathcal{N}(1, 0.25^2)$ and $y \sim \mathcal{N}(1, 0.75^2)$, respectively.
Here, CCC uses $y$ values to group data points into two clusters, and these clusters match the clusters obtained from $z$, yielding a statistically significant $c=0.30$.
