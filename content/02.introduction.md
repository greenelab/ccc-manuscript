## Introduction


The development of new technologies has vastly improved data collection and thus generated a deluge of information across different diciplines.
This provides new opportunities to address unanswered scientific questions [CITATION], provided we have both efficient and complex enough tools for the analysis.
For this purpose, correlation analysis is an essential statistical technique to discover relationships between variables [@pmid:21310971].
Correlation coefficients are used in many fundamental exporatory data mining techniques, such as a clustering or community detection, to compute a similarity value between a pair of objects of scientific interest such as genes [@pmid:27479844] or morpho-agronomic traits in crop plans [@doi:10.1093/bioinformatics/bty899].
Correlation methods were also successfuly used in supervised tasks for feature selection to improve prediction accuracy [@pmid:27006077; @pmid:33729976].
Coefficients such as Pearson or Spearman are obiquitous across different application domains and scientific areas, such as engineering or medicine, and even minor and significant improvements in these techniques could have enormous concequences in industry and research.


In transcriptomics, almost every single analysis starts with the correlation between a pair of genes, which can suggest their function [@pmid:21241896], aid in discovering common and cell lineage-specific regulatory networks [@pmid:25915600], and capture important important interactions in a living organism that can iluminate their function in other species [@pmid:21606319; @pmid:16968540].
The analysis of large RNA-seq datasets [@pmid:32913098; @pmid:34844637] can also reveal complex transcriptional mechanisms underlying human diseases [@pmid:27479844; @pmid:31121115; @pmid:30668570; @pmid:32424349; @pmid:34475573].
Since the introduction of the omnigenic model of complex traits [@pmid:28622505; @pmid:31051098], gene-gene relationships are playing an increasingly important role in genetic studies of diseases [@pmid:34845454; @doi:10.1101/2021.07.05.450786; @doi:10.1101/2021.10.21.21265342].
These very recent approaches combine disease-associated genes (via genome-wide association studies or GWAS) with gene co-expression networks to prioritize key genes not captured by standard methods, but potentially part of disease-relevant and highly-interconnected regulatory networks.


Standard correlation methods such as Pearson and Spearman are by far the most popular approaches, since they use easy-to-compute statistics and can therefore be quickly computed.
However, they can only capture linear or monotonic patterns, which might certeinly not be enough to detect complex relationships in data across different domains.
Novel approaches, such as Maximal Information Coefficient (MIC) [REF] or Distance Correlation (DC) [REF] can capture nonlinear patterns, but they are impractical not only for big data but also for even just moderately sized datasets.
We previously shown that Clustermatch, a method for cluster analysis on highly diverse datasets, significantly outperformed Pearson, Spearman, MIC and DC in detecting linear and nonlinear relationships with varying levels of noise [@doi:10.1093/bioinformatics/bty899].
Here we present the Clustermatch's correlation method, an efficient not-only-linear coefficient that can handle both quantitative and qualitative variables.
We applied Clustermatch to gene expression data from the Genotype-Tissue Expression (GTEx) project and showed that it can detect known linear and also novel nonlinear and biologically meaningful gene-gene patterns completely missed by standard coefficients.
<!-- TODO: Add something about GIANT results? -->
Clustermatch scores distribute very similarly to MIC, although it is much faster to compute and results are easy to interpret.
It also has a single parameter that controls the complexity of relationships of interest alongside computation time.
Our results show that Clutermatch is an attrative replacement for standard linear-only coefficients that can be readily used across different research areas.
Its ability to efficiently handle diverse data types (including numerical and categorical features) reduces preprocessing steps and makes it appealing for the analysis of large and complex repositories.
