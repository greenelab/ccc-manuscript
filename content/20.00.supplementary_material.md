## Supplementary material {.page_break_before}

### Supplementary Note 1: Comparison with the Maximal Information Coefficient (MIC) {#sec:mic}

#### Conceptual and statistical differences between CCC and MIC

The Clustermatch Correlation Coefficient (CCC) and the Maximal Information Coefficient (MIC) [@pmid:22174245] are measures designed to capture non-linear relationships between variables.
While they share certain similarities, there are also notable differences between them.

Consider two variables with $n$ data points on a scatterplot.
We can overlay a grid on this scatterplot with $x$ columns and $y$ rows, where each cell of this grid contains a portion of the data points, thereby defining a bivariate probability distribution.
The MIC algorithm seeks an optimal grid configuration that maximizes the ratio of mutual information $I$ to $\log \min { x, y }$, subject to the constraint that $xy < n^{0.6}$
This normalization process using $\log \min { x, y }$ scales the MIC score between zero and one.

<!-- The CCC and MIC [@pmid:22174245] are not-only-linear correlation coefficients that share some similarities, but also have important differences.
Given two variables with $n$ points, imagine a grid their scatterplot with $x$ columns and $y$ rows.
This grid, where their cells have different proportions of points, defines a bivariate probability distribution.
The MIC finds an optimal grid that maximizes $I / \log \min \{ x, y \}$ constrained by $xy < n^{0.6}$, where $I$ is the mutual information of the bivariate probability distribution, and $\log \min \{ x, y \}$ normalizes the MIC score between zero and one. -->

The CCC, as defined in [Methods](#sec:ccc_algo), also generates a symmetric, normalized score between zero and one.
However, unlike MIC which utilizes normalized mutual information, CCC employs the Adjusted Rand Index (ARI).
The ARI has an advantageous property: it consistently returns a baseline (zero) for independently drawn partitions, irrespective of the number of clusters.
This property is not inherent in mutual information, which can produce varied values for independent variables if the grid dimensions vary.
MIC mitigates this by limiting the grid size with the constraint $xy < n^{0.6}$.

<!-- From the definition of CCC in [Methods](#sec:ccc_algo), both CCC and MIC are symmetric and normalized between zero and one.
MIC uses a normalized mutual information score, whereas CCC uses the adjusted Rand index (ARI).
Whereas both are symmetric and normalized, the ARI also has the property of yielding a constant baseline (zero) for independently drawn partitions regardless of their number of clusters.
This is not the case for mutual information, which may yield different values for independent variables if the grids have different sizes.
MIC addresses this issue by constraining the grid size to $xy < n^{0.6}$. -->

Both CCC and MIC involve binning their respective input data vectors, aiming to maximize the mutual information and the ARI, respectively.
However, their approaches differ significantly in complexity and execution.
MIC utilizes a sophisticated dynamic programming algorithm to identify the optimal grid.
In contrast, CCC employs a more straightforward and faster method, partitioning the data points separately using the two vectors.
Our analysis on gene expression data (shown later), indicates that CCC's simpler method achieves comparable results to MIC.
While CCC might benefit from adopting MIC's more complex grid search approach, it remains uncertain if MIC could maintain its performance using CCC's simpler partitioning strategy.

<!-- Both CCC and MIC perform binning of the two input data vectors to maximize the mutual information and the ARI, respectively.
For this, however, MIC employs a complex dynamic programming algorithm to find the optimal grid, whereas CCC uses a simpler and faster approach by finding an optimal partition of the data points using the two vectors separately.
As we show next, this simpler approach of CCC yields essentially the same results as MIC in our gene expression data.
While it is very likely that CCC might benefit of the more complex grid search of MIC, it is not clear whether MIC would produce comparable results using the simpler partitioning approach of CCC. -->

Regarding their parameters, CCC's $k_{\mathrm{max}}$ (maximum number of clusters) and MIC's $B(n)$ (maximum grid size) serve similar purposes.
They control both the complexity of the patterns detected and the computational time.
For example, as illustrated in Figure @fig:datasets_rel (Anscombe I and III), a $k_{\mathrm{max}}$ of 2 is adequate for identifying linear patterns but insufficient for more complex patterns like quadratic or two-lines patterns.
A similar principle applies to MIC's $B(n)$.
However, a critical distinction exists between the two: the constant baseline property of ARIs ensures that CCC returns a value close to zero for independent variables, regardless of $k_{\mathrm{max}}$.
In contrast, MIC may produce non-zero scores for independent data if $B(n)$ is set too high, as discussed in section 2.2.1 of the supplementary material in [@pmid:22174245].
The authors of MIC suggest that a value of $B(n) = n^{0.6}$ is generally effective in practice.

<!-- Both coefficients have a conceptually similar parameter: $k_{\mathrm{max}}$ for CCC (the maximum number of clusters) and $B(n)$ for MIC (the maximum grid size).
These two parameters control the complexity of the patterns found and the computational time.
For instance, we show in Figure @fig:datasets_rel (Anscombe I and III) that setting $k_{\mathrm{max}}=2$ should be enough to find a linear pattern, whereas this value is clearly insufficient to capture more complex patterns (such as in the cases of quadratic or two-lines patterns).
The same idea applies to MIC and its parameter $B(n)$.
However, there is a key difference between the normalized mutual information used by MIC and the ARI used by CCC.
As shown in Figure @fig:constant_baseline:k_max, the constant baseline property of ARIs ensures that the CCC returns a value close to zero for independent variables regardless of the value of $k_{\mathrm{max}}$.
This is clearly not the case for MIC, where too high $B(n)$ values could yield non-zero MIC scores for independent data (see section 2.2.1 of the supplementary material in [@pmid:22174245]).
The authors of MIC found that a value of $B(n) = n^{0.6}$ works well in practice. -->

Conceptually, CCC is grounded in clustering input data using each variable separately, accommodating various types of variables, including both numerical and categorical variables without inherent ordering between categories (see [Methods](#sec:ccc_algo)).
MIC, however, is specifically designed for numerical variables.

<!-- Conceptually, CCC is based on clustering the input data using each variable separately.
Any type of variable that can be used to cluster a group of data points can be used in CCC, including numerical and categorical variables with no intrinsic order between the category values (see [Methods](#sec:ccc_algo)).
However, MIC is defined only for numerical variables. -->

- put here all that i mentioned to the reviewer

#### Comparison in gene expression data

We compared all the coefficients in this study with MIC, a popular nonlinear method that can find complex relationships in data, although very computationally intensive [@doi:10.1098/rsos.201424].
We ran MIC<sub>e</sub> (see Methods) on all possible pairwise comparisons of our 5,000 highly variable genes from whole blood in GTEx v8.
<!-- This took 4 days and 19 hours to finish (compared with 9 hours for CCC). -->
Then we performed the analysis on the distribution of coefficients (the same as in the main text), shown in Figure @fig:dist_coefs_mic.
We verified that CCC and MIC behave similarly in this dataset, with essentially the same distribution but only shifted.
Figure @{fig:dist_coefs_mic}c shows that these two coefficients relate almost linearly, and both compare very similarly with Pearson and Spearman.

![
**Distribution of MIC values on gene expression (GTEx v8, whole blood)  and comparison with other methods.**
**a)** Histogram of coefficient values.
**b)** Corresponding cumulative histogram. The dotted line maps the coefficient value that accumulates 70% of gene pairs.
**c)** 2D histogram plot with hexagonal bins between all coefficients, where a logarithmic scale was used to color each hexagon.
](images/coefs_comp/gtex_whole_blood/mic/dist-main.svg "Distribution of MIC values"){#fig:dist_coefs_mic tag="S1" width="100%"}


### Supplementary Note 2: Computational complexity of coefficients {#sec:time_test .page_break_before}

We also compared CCC with the other coefficients in terms of computational complexity.
Although CCC and MIC might identify similar gene pairs in gene expression data (see [here](#sec:mic)), the use of MIC in large datasets remains limited due to its very long computation time, despite some methodological/implementation improvements [@doi:10.1093/bioinformatics/bts707; @doi:10.1371/journal.pone.0157567; @doi:10.4137/EBO.S13121; @doi:10.1038/srep06662; @doi:10.1098/rsos.201424].
The original MIC implementation uses ApproxMaxMI, a computationally demanding heuristic estimator [@doi:10.1126/science.1205438].
Recently, a more efficient implementation called MIC<sub>e</sub> was proposed [@Reshef2016].
These two MIC estimators are provided by the `minepy` package [@doi:10.1093/bioinformatics/bts707], a C implementation available for Python.
We compared all these coefficients in terms of computation time on randomly generated variables of different sizes, which simulates a scenario of gene expression data with different numbers of conditions.
Differently from the rest, CCC allows us to easily parallelize the computation of a single gene pair (see [Methods](#sec:ccc_algo)), so we also tested the cases using 1 and 3 CPU cores.
Figure @fig:time_test shows the time in seconds in log scale.

![
**Computational complexity of all correlation coefficients on simulated data.**
We simulated variables/features with varying data sizes (from 100 to a million, $x$-axis).
The plot shows the average time in seconds (log-scale) taken for each coefficient on ten repetitions (1000 repetitions were performed for data size 100).
CCC was run using 1 and 3 CPU cores.
MIC and MIC<sub>e</sub> did not finish running in a reasonable amount of time for data sizes of 10,000 and 100,000, respectively.
](images/coefs_comp/time_test/time_test-main.svg "Computation time"){#fig:time_test tag="S2" width="55%"}

As we already expected, Pearson and Spearman were the fastest, given that they only need to compute basic summary statistics from the data.
For example, Pearson is three orders of magnitude faster than CCC.
Among the nonlinear coefficients, CCC was faster than the two MIC variations (up to two orders of magnitude), with the only exception in very small data sizes.
The difference is important because both MIC variants were implemented in C [@doi:10.1093/bioinformatics/bts707], a high-performance programming language, whereas CCC was implemented in Python (optimized with `numba`).
For a data size of a million, the multi-core CCC was twice as fast as the single-core CCC.
This suggests that new implementations using more advanced processing units (such as GPUs) are feasible and could make CCC reach speeds closer to Pearson.


### Figures {.page_break_before}


![
**The expression levels of *KDM6A* and *DDX3Y* display sex-specific associations across GTEx tissues.**
CCC captures this nonlinear relationship in all?? GTEx tissues (nine examples are shown in the first three rows), except in female-specific organs (last row).
](images/coefs_comp/kdm6a_vs_ddx3y/gtex-KDM6A_vs_DDX3Y-main.svg "KDM6A and DDX3Y across different GTEx tissues"){#fig:gtex_tissues:kdm6a_ddx3y width="95%" tag="S3"}


![
**Constant baseline property: CCC values are close to zero for random/independent variables.**
The plot shows CCC values for normally distributed and independent variables with different sizes $n$ and using different values for parameter $k_{\mathrm{max}}$ (maximum number of clusters).
](images/misc/constant_baseline-k_max.svg "Constant baseline property: CCC values are close to zero for random/independent variables"){#fig:constant_baseline:k_max width="65%" tag="S4"}


### Tables {.page_break_before}


| | **Interaction confidence** <!-- $colspan="7" -->    | | | | | | |
|:------:|:-----:|:-----:|:-----:|:--------:|:-----:|:-----:|:-----:|
| | **Blood** <!-- $colspan="3" --> | | | **Predicted cell type** <!-- $colspan="4" --> | | | |
| **Gene** |  **Min.** | **Avg.** | **Max.** |  **Cell type** | **Min.** | **Avg.** | **Max.** |
| *IFNG* | 0.19 | 0.42 | 0.54 | Natural killer cell<!-- $rowspan="2" --> | 0.74 | 0.90 | 0.99 |
| *SDS* | 0.18 | 0.29 | 0.41 | 0.65 | 0.81 | 0.94<!-- $removenext="2" --> |
| <!-- $colspan="7" --> |||||||
| *PRSS36* | 0.07 | 0.10 | 0.14 | Macrophage<!-- $rowspan="2" --> | 0.04 | 0.05 | 0.08 |
| *CCL18* | 0.07 | 0.74 | 0.86 | 0.05 | 0.69 | 0.90<!-- $removenext="2" --> |
| <!-- $colspan="7" --> |||||||
| *UTY* | 0.03 | 0.36 | 0.84 | Placenta<!-- $rowspan="2" --> | 0.01 | 0.03 | 0.04 |
| *KDM6A* | 0.03 | 0.42 | 0.58 | 0.04 | 0.38 | 0.61<!-- $removenext="2" --> |
| <!-- $colspan="7" --> |||||||
| *DDX3Y* | 0.05 | 0.33 | 0.78 | Testis<!-- $rowspan="2" --> | 0.07 | 0.11 | 0.18 |
| *KDM6A* | 0.43 | 0.51 | 0.58 | 0.27 | 0.34 | 0.48<!-- $removenext="2" --> |
| <!-- $colspan="7" --> |||||||
| *RASSF2* | 0.69 | 0.77 | 0.90 | Leukocyte<!-- $rowspan="2" --> | 0.66 | 0.74 | 0.88 |
| *CYTIP* | 0.74 | 0.85 | 0.91 | 0.76 | 0.84 | 0.91<!-- $removenext="2" --> |
| <!-- $colspan="7" --> |||||||
| *MYOZ1* | 0.09 | 0.17 | 0.37 | Skeletal muscle<!-- $rowspan="2" --> | 0.11 | 0.11 | 0.12 |
| *TNNI2* | 0.10 | 0.22 | 0.44 | 0.10 | 0.11 | 0.12<!-- $removenext="2" --> |
| <!-- $colspan="7" --> |||||||
| *SCGB3A1* | 0.16 | 0.19 | 0.23 | Placenta<!-- $rowspan="2" --> | 0.11 | 0.11 | 0.12 |
| *C19orf33* | 0.15 | 0.19 | 0.28 | 0.11 | 0.12 | 0.17<!-- $removenext="2" --> |

Table: **Network statistics of seven gene pairs shown in Figure @{fig:upsetplot_coefs}b for blood and predicted cell types.**
Only gene pairs present in GIANT models are listed.
For each gene in the pair (first column), the minimum, average and maximum interaction coefficients with the other genes in the network are shown.
{#tbl:giant:weights tag="S1"}
