## Supplementary material {.page_break_before}

### Supplementary Note 1: Comparison with the Maximal Information Coefficient (MIC) {#sec:mic}

#### Conceptual and statistical differences between CCC and MIC

The Clustermatch Correlation Coefficient (CCC) and the Maximal Information Coefficient (MIC) [@pmid:22174245] are measures designed to capture non-linear relationships between variables.
While they share certain similarities, there are also notable differences between them.

Consider two variables with $n$ data points on a scatterplot.
We can overlay a grid on this scatterplot with $x$ columns and $y$ rows, where each cell of this grid contains a portion of the data points, thereby defining a bivariate probability distribution.
The MIC algorithm seeks an optimal grid configuration that maximizes the ratio of mutual information $I$ to $\log \min { x, y }$, subject to the constraint that $xy < n^{0.6}$
This normalization process using $\log \min { x, y }$ scales the MIC score between zero and one.

The CCC, as defined in [Methods](#sec:ccc_algo), also generates a symmetric, normalized score between zero and one.
However, unlike MIC which utilizes normalized mutual information, CCC employs the Adjusted Rand Index (ARI).
The ARI has an advantageous property: it consistently returns a baseline (zero) for independently drawn partitions, irrespective of the number of clusters (see Figure @fig:constant_baseline:k_max).
This property is not inherent in mutual information, which can produce varied values for independent variables if the grid dimensions vary.
MIC mitigates this by limiting the grid size with the constraint $xy < n^{0.6}$.

Both CCC and MIC involve binning the input data vectors, aiming to maximize the mutual information and the ARI, respectively.
However, their approaches differ significantly in complexity and execution.
MIC utilizes a sophisticated dynamic programming algorithm to identify the optimal grid.
In contrast, CCC employs a more straightforward and faster method, partitioning the data points separately using the two vectors.
Our analysis on gene expression data (shown later), indicates that CCC's simpler method achieves comparable results to MIC.
While CCC might benefit from adopting MIC's more complex grid search approach, it remains uncertain if MIC could maintain its performance using CCC's simpler partitioning strategy.

Regarding their parameters, CCC's $k_{\mathrm{max}}$ (maximum number of clusters) and MIC's $B(n)$ (maximum grid size) serve similar purposes.
They control both the complexity of the patterns detected and the computational time.
For example, as illustrated in Figure @fig:datasets_rel (Anscombe I and III), a $k_{\mathrm{max}}$ of 2 is adequate for identifying linear patterns but insufficient for more complex patterns like quadratic or two-lines patterns.
A similar principle applies to MIC's $B(n)$.
However, a critical distinction exists between the two: the constant baseline property of ARIs ensures that CCC returns a value close to zero for independent variables, regardless of $k_{\mathrm{max}}$.
In contrast, MIC may produce non-zero scores for independent data if $B(n)$ is set too high, as discussed in section 2.2.1 of the supplementary material in [@pmid:22174245].
The authors of MIC suggest that a value of $B(n) = n^{0.6}$ is generally effective in practice.

Conceptually, CCC is grounded in clustering input data using each variable separately.
This process effectively transforms each variable into a set of partitions, each containing a different number of clusters.
The CCC then quantifies the correlation between variables by assessing the similarity of these partitions.
This allows to process various types of variables, including both numerical and categorical variables, even when the categories are nominal (i.e., they lack intrinsic order), as explained in [Methods](#sec:ccc_algo).
MIC, however, is specifically designed for numerical variables.

<!-- - put here all that i mentioned to the reviewer -->

#### Comparison in gene expression data

We compared all the coefficients in this study with MIC, a popular nonlinear method that can find complex relationships in data, although very computationally intensive [@doi:10.1098/rsos.201424].
We ran MIC<sub>e</sub> (see Methods) on all possible pairwise comparisons of our 5,000 highly variable genes from whole blood in GTEx v8.
<!-- This took 4 days and 19 hours to finish (compared with 9 hours for CCC). -->
Then we performed the analysis on the distribution of coefficients (the same as in the main text), shown in Figure @fig:dist_coefs_mic.
We verified that CCC and MIC behave similarly in this dataset, with essentially the same distribution but only shifted.
Figure @{fig:dist_coefs_mic}c shows that these two coefficients relate almost linearly, and both compare very similarly with Pearson and Spearman.

![
**Distribution of MIC values on gene expression (GTEx v8, whole blood)  and comparison with other methods.**
**a)** Histogram of coefficient values.
**b)** Corresponding cumulative histogram. The dotted line maps the coefficient value that accumulates 70% of gene pairs.
**c)** 2D histogram plot with hexagonal bins between all coefficients, where a logarithmic scale was used to color each hexagon.
](images/coefs_comp/gtex_whole_blood/mic/dist-main.svg "Distribution of MIC values"){#fig:dist_coefs_mic tag="S1" width="100%"}


### Supplementary Note 2: Computational complexity of coefficients {#sec:time_test .page_break_before}

We also compared CCC with the other coefficients in terms of computational complexity.
Although CCC and MIC might identify similar gene pairs in gene expression data (see [here](#sec:mic)), the use of MIC in large datasets remains limited due to its very long computation time, despite some methodological/implementation improvements [@doi:10.1093/bioinformatics/bts707; @doi:10.1371/journal.pone.0157567; @doi:10.4137/EBO.S13121; @doi:10.1038/srep06662; @doi:10.1098/rsos.201424].
The original MIC implementation uses ApproxMaxMI, a computationally demanding heuristic estimator [@doi:10.1126/science.1205438].
Recently, a more efficient implementation called MIC<sub>e</sub> was proposed [@Reshef2016].
These two MIC estimators are provided by the `minepy` package [@doi:10.1093/bioinformatics/bts707], a C implementation available for Python.
We compared all these coefficients in terms of computation time on randomly generated variables of different sizes, which simulates a scenario of gene expression data with different numbers of conditions.
Differently from the rest, CCC allows us to easily parallelize the computation of a single gene pair (see [Methods](#sec:ccc_algo)), so we also tested the cases using 1 and 3 CPU cores.
Figure @fig:time_test shows the time in seconds in log scale.

![
**Computational complexity of all correlation coefficients on simulated data.**
We simulated variables/features with varying data sizes (from 100 to a million, $x$-axis).
The plot shows the average time in seconds (log-scale) taken for each coefficient on ten repetitions (1000 repetitions were performed for data size 100).
CCC was run using 1 and 3 CPU cores.
MIC and MIC<sub>e</sub> did not finish running in a reasonable amount of time for data sizes of 10,000 and 100,000, respectively.
](images/coefs_comp/time_test/time_test-main.svg "Computation time"){#fig:time_test tag="S2" width="55%"}

As we already expected, Pearson and Spearman were the fastest, given that they only need to compute basic summary statistics from the data.
For example, Pearson is three orders of magnitude faster than CCC.
Among the nonlinear coefficients, CCC was faster than the two MIC variations (up to two orders of magnitude), with the only exception in very small data sizes.
The difference is important because both MIC variants were implemented in C [@doi:10.1093/bioinformatics/bts707], a high-performance programming language, whereas CCC was implemented in Python (optimized with `numba`).
For a data size of a million, the multi-core CCC was twice as fast as the single-core CCC.
This suggests that new implementations using more advanced processing units (such as GPUs) are feasible and could make CCC reach speeds closer to Pearson.


### Figures {.page_break_before}


![
**The expression levels of *KDM6A* and *DDX3Y* display sex-specific associations across GTEx tissues.**
CCC captures this nonlinear relationship in all?? GTEx tissues (nine examples are shown in the first three rows), except in female-specific organs (last row).
](images/coefs_comp/kdm6a_vs_ddx3y/gtex-KDM6A_vs_DDX3Y-main.svg "KDM6A and DDX3Y across different GTEx tissues"){#fig:gtex_tissues:kdm6a_ddx3y width="95%" tag="S3"}


![
**Constant baseline property: CCC values are close to zero for random/independent variables.**
The plot shows CCC values for normally distributed and independent variables with different sizes $n$ and using different values for parameter $k_{\mathrm{max}}$ (maximum number of clusters).
](images/misc/constant_baseline-k_max.svg "Constant baseline property: CCC values are close to zero for random/independent variables"){#fig:constant_baseline:k_max width="65%" tag="S4"}


### Tables {.page_break_before}


| | **Interaction confidence** <!-- $colspan="7" -->    | | | | | | |
|:------:|:-----:|:-----:|:-----:|:--------:|:-----:|:-----:|:-----:|
| | **Blood** <!-- $colspan="3" --> | | | **Predicted cell type** <!-- $colspan="4" --> | | | |
| **Gene** |  **Min.** | **Avg.** | **Max.** |  **Cell type** | **Min.** | **Avg.** | **Max.** |
| *IFNG* | 0.19 | 0.42 | 0.54 | Natural killer cell<!-- $rowspan="2" --> | 0.74 | 0.90 | 0.99 |
| *SDS* | 0.18 | 0.29 | 0.41 | 0.65 | 0.81 | 0.94<!-- $removenext="2" --> |
| <!-- $colspan="7" --> |||||||
| *PRSS36* | 0.07 | 0.10 | 0.14 | Macrophage<!-- $rowspan="2" --> | 0.04 | 0.05 | 0.08 |
| *CCL18* | 0.07 | 0.74 | 0.86 | 0.05 | 0.69 | 0.90<!-- $removenext="2" --> |
| <!-- $colspan="7" --> |||||||
| *UTY* | 0.03 | 0.36 | 0.84 | Placenta<!-- $rowspan="2" --> | 0.01 | 0.03 | 0.04 |
| *KDM6A* | 0.03 | 0.42 | 0.58 | 0.04 | 0.38 | 0.61<!-- $removenext="2" --> |
| <!-- $colspan="7" --> |||||||
| *DDX3Y* | 0.05 | 0.33 | 0.78 | Testis<!-- $rowspan="2" --> | 0.07 | 0.11 | 0.18 |
| *KDM6A* | 0.43 | 0.51 | 0.58 | 0.27 | 0.34 | 0.48<!-- $removenext="2" --> |
| <!-- $colspan="7" --> |||||||
| *RASSF2* | 0.69 | 0.77 | 0.90 | Leukocyte<!-- $rowspan="2" --> | 0.66 | 0.74 | 0.88 |
| *CYTIP* | 0.74 | 0.85 | 0.91 | 0.76 | 0.84 | 0.91<!-- $removenext="2" --> |
| <!-- $colspan="7" --> |||||||
| *MYOZ1* | 0.09 | 0.17 | 0.37 | Skeletal muscle<!-- $rowspan="2" --> | 0.11 | 0.11 | 0.12 |
| *TNNI2* | 0.10 | 0.22 | 0.44 | 0.10 | 0.11 | 0.12<!-- $removenext="2" --> |
| <!-- $colspan="7" --> |||||||
| *SCGB3A1* | 0.16 | 0.19 | 0.23 | Placenta<!-- $rowspan="2" --> | 0.11 | 0.11 | 0.12 |
| *C19orf33* | 0.15 | 0.19 | 0.28 | 0.11 | 0.12 | 0.17<!-- $removenext="2" --> |

Table: **Network statistics of seven gene pairs shown in Figure @{fig:upsetplot_coefs}b for blood and predicted cell types.**
Only gene pairs present in GIANT models are listed.
For each gene in the pair (first column), the minimum, average and maximum interaction coefficients with the other genes in the network are shown.
{#tbl:giant:weights tag="S1"}
