## Methods

The code and data necessary to reproduce our analyses and generate the figures are available at [https://github.com/greenelab/ccc](https://github.com/greenelab/ccc).
We provide scripts to download the data and run all the steps, as well as a Docker image to ensure the same runtime environment.


### The CCC algorithm {#sec:ccc_algo .page_break_before}

The Clustermatch Correlation Coefficient (CCC) computes a similarity value $c \in \left[0,1\right]$ between any pair of numerical or categorical features/variables $\mathbf{x}$ and $\mathbf{y}$ measured on $n$ objects.
CCC assumes that if two features $\mathbf{x}$ and $\mathbf{y}$ are similar, then the partitioning by clustering of the $n$ objects using each feature separately should match.
For example, given $\mathbf{x}=(11, 27, 32, 40)$ and $\mathbf{y}=10x=(110, 270, 320, 400)$, where $n=4$, partitioning each variable into two clusters ($k=2$) using their medians (29.5 for $\mathbf{x}$ and 295 for $\mathbf{y}$) would result in partition $\Omega^{\mathbf{x}}_{k=2}=(1, 1, 2, 2)$ for $\mathbf{x}$, and partition $\Omega^{\mathbf{y}}_{k=2}=(1, 1, 2, 2)$ for $\mathbf{y}$.
Then, the agreement between $\Omega^{\mathbf{x}}_{k=2}$ and $\Omega^{\mathbf{y}}_{k=2}$ can be computed using a measure of similarity between partitions, such as the adjusted Rand index (ARI) [@doi:10.1007/BF01908075].
In that case, it will return the maximum value (1.0 in the case of ARI). 

Note that the same value of $k$ might not be the right one to find a relationship between any two features.
For instance, in the quadratic example in Figure @fig:datasets_rel, CCC returns a value of 0.36 (grouping objects in four clusters using one feature and two using the other).
If we used only two clusters instead, CCC would return a similarity value of 0.02.
Therefore, the CCC algorithm searches for the optimal number of clusters given a maximum $k$, which is its single parameter $k_{\mathrm{max}}$.

![
](images/intro/ccc_algorithm/ccc_algorithm.svg "CCC algorithm"){width="75%"}

The algorithm `ccc` generates a list of partitionings for each of the features $\mathbf{x}$ and $\mathbf{y}$.
It then computes the Adjusted Rand Index (ARI) between each partition in $\Omega^{\mathbf{x}}$ and $\Omega^{\mathbf{y}}$.
The pair with the maximum ARI is kept, and the Correlation Coefficient (CCC) is returned as a value between 0 and 1.
This is because ARI can return negative values, which are not meaningful in this context.


Interestingly, CCC only needs a pair of partitions to compute a similarity value.
This means that any type of feature that can be used to perform clustering/grouping is supported.
For numerical features (lines 2 to 5 in the `get_partitions` function), quantiles are used for clustering.
For example, the median can generate $k=2$ clusters of objects, ranging from $k=2$ to $k=k_{\mathrm{max}}$.
For categorical features (lines 7 to 9), the categories are used to group objects together.
This means that numerical and categorical variables can be naturally integrated, since clusters do not need an order.


For all our analyses, we used $k_{\mathrm{max}}=10$.
This means that for each gene pair, 18 partitions were generated (9 for each gene, from $k=2$ to $k=10$), and 81 ARI comparisons were performed.
To reduce computation time, smaller values of $k_{\mathrm{max}}$ can be used, although this may lead to missing more complex/general relationships.
Our examples in Figure @fig:datasets_rel suggest that using $k_{\mathrm{max}}=2$ would force CCC to find linear-only patterns, which could be a valid use case scenario when only this kind of relationships are desired.
In addition, $k_{\mathrm{max}}=2$ implies that only two partitions are generated, and only one ARI comparison is performed.
Our Python implementation of CCC provides flexibility in specifying $k_{\mathrm{max}}$.
For instance, instead of the maximum $k$ (an integer), the parameter could be a custom list of integers: for example, `[2, 5, 10]` will partition the data into two, five and ten clusters.


For our study, we used three CPU cores to speed up the computation of the correlation coefficient.
This improved the performance of our analysis.
To further increase efficiency, a future implementation of the coefficient could use graphical processing units (GPUs).


We implemented an efficient correlation coefficient based on machine learning, named CCC.
It is optimized with `numba` [@doi:10.1145/2833157.2833162] and its Python implementation is available in our Github repository [@url:https://github.com/greenelab/clustermatch-gene-expr].
Additionally, a package is published in the Python Package Index (PyPI) for easy installation.
